{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import Initializer\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/ordered_molecules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>molecule</th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.27497</td>\n",
       "      <td>0.22765</td>\n",
       "      <td>0.40548</td>\n",
       "      <td>1.185458</td>\n",
       "      <td>-1.527830</td>\n",
       "      <td>-4.614324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.13410</td>\n",
       "      <td>1.61428</td>\n",
       "      <td>1.62211</td>\n",
       "      <td>-4.601109</td>\n",
       "      <td>-0.659241</td>\n",
       "      <td>1.090967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12.05408</td>\n",
       "      <td>12.60878</td>\n",
       "      <td>2.90295</td>\n",
       "      <td>4.530882</td>\n",
       "      <td>-2.598597</td>\n",
       "      <td>2.008448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.85376</td>\n",
       "      <td>1.93829</td>\n",
       "      <td>4.87515</td>\n",
       "      <td>3.975239</td>\n",
       "      <td>0.917088</td>\n",
       "      <td>-2.261618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12.70257</td>\n",
       "      <td>0.12956</td>\n",
       "      <td>6.09076</td>\n",
       "      <td>-1.861295</td>\n",
       "      <td>-0.908830</td>\n",
       "      <td>5.337542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  molecule        px        py       pz        ax        ay        az\n",
       "0     0         1   0.27497   0.22765  0.40548  1.185458 -1.527830 -4.614324\n",
       "1     0         2   2.13410   1.61428  1.62211 -4.601109 -0.659241  1.090967\n",
       "2     0         3  12.05408  12.60878  2.90295  4.530882 -2.598597  2.008448\n",
       "3     0         4   0.85376   1.93829  4.87515  3.975239  0.917088 -2.261618\n",
       "4     0         5  12.70257   0.12956  6.09076 -1.861295 -0.908830  5.337542"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.iloc[:,2:] = scale(raw_data.iloc[:,2:], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_scaled = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>molecule</th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.644088</td>\n",
       "      <td>-1.687248</td>\n",
       "      <td>-1.622801</td>\n",
       "      <td>0.433043</td>\n",
       "      <td>-0.557533</td>\n",
       "      <td>-1.614066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.146260</td>\n",
       "      <td>-1.315603</td>\n",
       "      <td>-1.297500</td>\n",
       "      <td>-1.680760</td>\n",
       "      <td>-0.240568</td>\n",
       "      <td>0.381615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.510062</td>\n",
       "      <td>1.631154</td>\n",
       "      <td>-0.955030</td>\n",
       "      <td>1.655108</td>\n",
       "      <td>-0.948277</td>\n",
       "      <td>0.702545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.489103</td>\n",
       "      <td>-1.228761</td>\n",
       "      <td>-0.427706</td>\n",
       "      <td>1.452135</td>\n",
       "      <td>0.334665</td>\n",
       "      <td>-0.791101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.683712</td>\n",
       "      <td>-1.713539</td>\n",
       "      <td>-0.102678</td>\n",
       "      <td>-0.679920</td>\n",
       "      <td>-0.331648</td>\n",
       "      <td>1.867045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  molecule        px        py        pz        ax        ay        az\n",
       "0     0         1 -1.644088 -1.687248 -1.622801  0.433043 -0.557533 -1.614066\n",
       "1     0         2 -1.146260 -1.315603 -1.297500 -1.680760 -0.240568  0.381615\n",
       "2     0         3  1.510062  1.631154 -0.955030  1.655108 -0.948277  0.702545\n",
       "3     0         4 -1.489103 -1.228761 -0.427706  1.452135  0.334665 -0.791101\n",
       "4     0         5  1.683712 -1.713539 -0.102678 -0.679920 -0.331648  1.867045"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = raw_data_scaled.iloc[128:,2:]\n",
    "data = pd.concat([raw_data_scaled.iloc[0:255872,:].reset_index(drop=True), target_data.reset_index(drop=True)], axis = 1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.644088</td>\n",
       "      <td>-1.687248</td>\n",
       "      <td>-1.622801</td>\n",
       "      <td>0.433043</td>\n",
       "      <td>-0.557533</td>\n",
       "      <td>-1.614066</td>\n",
       "      <td>-1.643419</td>\n",
       "      <td>-1.687447</td>\n",
       "      <td>-1.623753</td>\n",
       "      <td>0.428789</td>\n",
       "      <td>-0.562693</td>\n",
       "      <td>-1.620018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.146260</td>\n",
       "      <td>-1.315603</td>\n",
       "      <td>-1.297500</td>\n",
       "      <td>-1.680760</td>\n",
       "      <td>-0.240568</td>\n",
       "      <td>0.381615</td>\n",
       "      <td>-1.146375</td>\n",
       "      <td>-1.313517</td>\n",
       "      <td>-1.298369</td>\n",
       "      <td>-1.706684</td>\n",
       "      <td>-0.293925</td>\n",
       "      <td>0.372428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.510062</td>\n",
       "      <td>1.631154</td>\n",
       "      <td>-0.955030</td>\n",
       "      <td>1.655108</td>\n",
       "      <td>-0.948277</td>\n",
       "      <td>0.702545</td>\n",
       "      <td>1.510726</td>\n",
       "      <td>1.631180</td>\n",
       "      <td>-0.957386</td>\n",
       "      <td>1.618997</td>\n",
       "      <td>-0.952184</td>\n",
       "      <td>0.774350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.489103</td>\n",
       "      <td>-1.228761</td>\n",
       "      <td>-0.427706</td>\n",
       "      <td>1.452135</td>\n",
       "      <td>0.334665</td>\n",
       "      <td>-0.791101</td>\n",
       "      <td>-1.487831</td>\n",
       "      <td>-1.228078</td>\n",
       "      <td>-0.426671</td>\n",
       "      <td>1.405047</td>\n",
       "      <td>0.302387</td>\n",
       "      <td>-0.800858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.683712</td>\n",
       "      <td>-1.713539</td>\n",
       "      <td>-0.102678</td>\n",
       "      <td>-0.679920</td>\n",
       "      <td>-0.331648</td>\n",
       "      <td>1.867045</td>\n",
       "      <td>1.683602</td>\n",
       "      <td>-1.713879</td>\n",
       "      <td>-0.101376</td>\n",
       "      <td>-0.654165</td>\n",
       "      <td>-0.308364</td>\n",
       "      <td>1.838234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1         2         3         4         5         6         7   \\\n",
       "0   0   1 -1.644088 -1.687248 -1.622801  0.433043 -0.557533 -1.614066   \n",
       "1   0   2 -1.146260 -1.315603 -1.297500 -1.680760 -0.240568  0.381615   \n",
       "2   0   3  1.510062  1.631154 -0.955030  1.655108 -0.948277  0.702545   \n",
       "3   0   4 -1.489103 -1.228761 -0.427706  1.452135  0.334665 -0.791101   \n",
       "4   0   5  1.683712 -1.713539 -0.102678 -0.679920 -0.331648  1.867045   \n",
       "\n",
       "         8         9         10        11        12        13  \n",
       "0 -1.643419 -1.687447 -1.623753  0.428789 -0.562693 -1.620018  \n",
       "1 -1.146375 -1.313517 -1.298369 -1.706684 -0.293925  0.372428  \n",
       "2  1.510726  1.631180 -0.957386  1.618997 -0.952184  0.774350  \n",
       "3 -1.487831 -1.228078 -0.426671  1.405047  0.302387 -0.800858  \n",
       "4  1.683602 -1.713879 -0.101376 -0.654165 -0.308364  1.838234  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_x = data.iloc[:,8] - data.iloc[:,2]\n",
    "diff_y = data.iloc[:,9] - data.iloc[:,3]\n",
    "diff_z = data.iloc[:,10] - data.iloc[:,4]\n",
    "\n",
    "\n",
    "diff_x_idx, = np.where(abs(diff_x) > 3)\n",
    "diff_y_idx, = np.where(abs(diff_y) > 3)\n",
    "diff_z_idx, = np.where(abs(diff_z) > 3)\n",
    "\n",
    "diff_idx = set(diff_x_idx) | set(diff_y_idx) | set(diff_z_idx)\n",
    "\n",
    "final_data = data.loc[data.index.drop(list(diff_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(final_data.iloc[:,2:8], \n",
    "                                                    final_data.iloc[:,8:], \n",
    "                                                    test_size = 0.3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(32, input_dim=6, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = regularizers.l2(0.01)))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = regularizers.l2(0.01)))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = regularizers.l2(0.01)))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(Dense(6, activation='linear', kernel_initializer = 'glorot_normal', kernel_regularizer = regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='mean_absolute_error', optimizer= \"rmsprop\", metrics=['mae'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "model_check_point = ModelCheckpoint('./model/DNN_BN.h5', monitor = 'val_loss', verbose=2, save_best_only=True)\n",
    "callback_list = [model_check_point, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178881 samples, validate on 76664 samples\n",
      "Epoch 1/150\n",
      "178881/178881 [==============================] - 10s 54us/step - loss: 0.3675 - mean_absolute_error: 0.1742 - val_loss: 0.1307 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13073, saving model to ./model/DNN_BN.h5\n",
      "Epoch 2/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1644 - mean_absolute_error: 0.1422 - val_loss: 0.1192 - val_mean_absolute_error: 0.0996\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13073 to 0.11923, saving model to ./model/DNN_BN.h5\n",
      "Epoch 3/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1548 - mean_absolute_error: 0.1368 - val_loss: 0.0870 - val_mean_absolute_error: 0.0701\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11923 to 0.08697, saving model to ./model/DNN_BN.h5\n",
      "Epoch 4/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1492 - mean_absolute_error: 0.1334 - val_loss: 0.0703 - val_mean_absolute_error: 0.0557\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08697 to 0.07035, saving model to ./model/DNN_BN.h5\n",
      "Epoch 5/150\n",
      "178881/178881 [==============================] - 9s 48us/step - loss: 0.1433 - mean_absolute_error: 0.1294 - val_loss: 0.0744 - val_mean_absolute_error: 0.0611\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07035\n",
      "Epoch 6/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1368 - mean_absolute_error: 0.1240 - val_loss: 0.0822 - val_mean_absolute_error: 0.0697\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07035\n",
      "Epoch 7/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1327 - mean_absolute_error: 0.1203 - val_loss: 0.0675 - val_mean_absolute_error: 0.0553\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07035 to 0.06746, saving model to ./model/DNN_BN.h5\n",
      "Epoch 8/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1312 - mean_absolute_error: 0.1193 - val_loss: 0.0675 - val_mean_absolute_error: 0.0558\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06746\n",
      "Epoch 9/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1299 - mean_absolute_error: 0.1182 - val_loss: 0.0757 - val_mean_absolute_error: 0.0644\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06746\n",
      "Epoch 10/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1300 - mean_absolute_error: 0.1186 - val_loss: 0.0757 - val_mean_absolute_error: 0.0644\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06746\n",
      "Epoch 11/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1303 - mean_absolute_error: 0.1191 - val_loss: 0.0669 - val_mean_absolute_error: 0.0557\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06746 to 0.06689, saving model to ./model/DNN_BN.h5\n",
      "Epoch 12/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1294 - mean_absolute_error: 0.1183 - val_loss: 0.0612 - val_mean_absolute_error: 0.0503\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06689 to 0.06119, saving model to ./model/DNN_BN.h5\n",
      "Epoch 13/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1301 - mean_absolute_error: 0.1193 - val_loss: 0.0605 - val_mean_absolute_error: 0.0500\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06119 to 0.06055, saving model to ./model/DNN_BN.h5\n",
      "Epoch 14/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1293 - mean_absolute_error: 0.1187 - val_loss: 0.0624 - val_mean_absolute_error: 0.0519\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06055\n",
      "Epoch 15/150\n",
      "178881/178881 [==============================] - 9s 48us/step - loss: 0.1296 - mean_absolute_error: 0.1191 - val_loss: 0.0591 - val_mean_absolute_error: 0.0486\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06055 to 0.05912, saving model to ./model/DNN_BN.h5\n",
      "Epoch 16/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1291 - mean_absolute_error: 0.1188 - val_loss: 0.0591 - val_mean_absolute_error: 0.0487\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05912 to 0.05909, saving model to ./model/DNN_BN.h5\n",
      "Epoch 17/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1287 - mean_absolute_error: 0.1184 - val_loss: 0.0607 - val_mean_absolute_error: 0.0505\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.05909\n",
      "Epoch 18/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1281 - mean_absolute_error: 0.1179 - val_loss: 0.0706 - val_mean_absolute_error: 0.0605\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05909\n",
      "Epoch 19/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1285 - mean_absolute_error: 0.1185 - val_loss: 0.0681 - val_mean_absolute_error: 0.0582\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.05909\n",
      "Epoch 20/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1280 - mean_absolute_error: 0.1179 - val_loss: 0.0768 - val_mean_absolute_error: 0.0668\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05909\n",
      "Epoch 21/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1285 - mean_absolute_error: 0.1185 - val_loss: 0.0587 - val_mean_absolute_error: 0.0487\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05909 to 0.05867, saving model to ./model/DNN_BN.h5\n",
      "Epoch 22/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1274 - mean_absolute_error: 0.1175 - val_loss: 0.0525 - val_mean_absolute_error: 0.0425\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05867 to 0.05252, saving model to ./model/DNN_BN.h5\n",
      "Epoch 23/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1281 - mean_absolute_error: 0.1182 - val_loss: 0.0591 - val_mean_absolute_error: 0.0494\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.05252\n",
      "Epoch 24/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1286 - mean_absolute_error: 0.1189 - val_loss: 0.0585 - val_mean_absolute_error: 0.0486\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.05252\n",
      "Epoch 25/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1276 - mean_absolute_error: 0.1179 - val_loss: 0.0772 - val_mean_absolute_error: 0.0675\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05252\n",
      "Epoch 26/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1280 - mean_absolute_error: 0.1184 - val_loss: 0.0594 - val_mean_absolute_error: 0.0498\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.05252\n",
      "Epoch 27/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1277 - mean_absolute_error: 0.1181 - val_loss: 0.0560 - val_mean_absolute_error: 0.0463\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05252\n",
      "Epoch 28/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1274 - mean_absolute_error: 0.1178 - val_loss: 0.0659 - val_mean_absolute_error: 0.0565\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05252\n",
      "Epoch 29/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1274 - mean_absolute_error: 0.1179 - val_loss: 0.0579 - val_mean_absolute_error: 0.0484\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.05252\n",
      "Epoch 30/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1275 - mean_absolute_error: 0.1180 - val_loss: 0.0614 - val_mean_absolute_error: 0.0520\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.05252\n",
      "Epoch 31/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1278 - mean_absolute_error: 0.1184 - val_loss: 0.0596 - val_mean_absolute_error: 0.0502\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05252\n",
      "Epoch 32/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1274 - mean_absolute_error: 0.1180 - val_loss: 0.0566 - val_mean_absolute_error: 0.0472\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.05252\n",
      "Epoch 33/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1278 - mean_absolute_error: 0.1184 - val_loss: 0.0541 - val_mean_absolute_error: 0.0447\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.05252\n",
      "Epoch 34/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1272 - mean_absolute_error: 0.1179 - val_loss: 0.0668 - val_mean_absolute_error: 0.0577\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05252\n",
      "Epoch 35/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1264 - mean_absolute_error: 0.1172 - val_loss: 0.0650 - val_mean_absolute_error: 0.0558\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.05252\n",
      "Epoch 36/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1286 - mean_absolute_error: 0.1195 - val_loss: 0.0644 - val_mean_absolute_error: 0.0553\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05252\n",
      "Epoch 37/150\n",
      "178881/178881 [==============================] - 8s 47us/step - loss: 0.1269 - mean_absolute_error: 0.1177 - val_loss: 0.0648 - val_mean_absolute_error: 0.0556\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05252\n",
      "Epoch 38/150\n",
      "178881/178881 [==============================] - 9s 48us/step - loss: 0.1269 - mean_absolute_error: 0.1178 - val_loss: 0.0639 - val_mean_absolute_error: 0.0548\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05252\n",
      "Epoch 39/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1273 - mean_absolute_error: 0.1181 - val_loss: 0.0622 - val_mean_absolute_error: 0.0531\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.05252\n",
      "Epoch 40/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1257 - mean_absolute_error: 0.1166 - val_loss: 0.0770 - val_mean_absolute_error: 0.0679\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05252\n",
      "Epoch 41/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1270 - mean_absolute_error: 0.1179 - val_loss: 0.0717 - val_mean_absolute_error: 0.0626\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.05252\n",
      "Epoch 42/150\n",
      "178881/178881 [==============================] - 8s 46us/step - loss: 0.1267 - mean_absolute_error: 0.1177 - val_loss: 0.0707 - val_mean_absolute_error: 0.0616\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.05252\n"
     ]
    }
   ],
   "source": [
    "hist_1 = model_1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, batch_size=64, callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./model/DNN_BN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae_metric = mean_absolute_error(y_test, y_pred)\n",
    "r2_error = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0030698972861182374\n",
      "RMSE:  0.05540665380726612\n",
      "MAE:  0.04252060348393475\n",
      "R2_Score: 0.996926668798602\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae_metric)\n",
    "print(\"R2_Score:\", r2_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax1 = fig.add_subplot(3,3,1)\n",
    "ax2 = fig.add_subplot(3,3,2)\n",
    "ax3 = fig.add_subplot(3,3,3)\n",
    "ax4 = fig.add_subplot(3,3,4)\n",
    "ax5 = fig.add_subplot(3,3,5)\n",
    "ax6 = fig.add_subplot(3,3,6)\n",
    "\n",
    "ax1.scatter(y_test.values[:,0], y_pred[:,0], s= 2)\n",
    "ax1.set_xlabel('x_real')\n",
    "ax1.set_ylabel('x_pred')\n",
    "ax2.scatter(y_test.values[:,1], y_pred[:,1], s= 2)\n",
    "ax2.set_xlabel('y_real')\n",
    "ax2.set_ylabel('y_pred')\n",
    "ax3.scatter(y_test.values[:,2], y_pred[:,2], s= 2)\n",
    "ax3.set_xlabel('z_real')\n",
    "ax3.set_ylabel('z_pred')\n",
    "ax4.scatter(y_test.values[:,3], y_pred[:,3], s= 2)\n",
    "ax4.set_xlabel('ax_real')\n",
    "ax4.set_ylabel('ax_pred')\n",
    "ax5.scatter(y_test.values[:,4], y_pred[:,4], s= 2)\n",
    "ax5.set_xlabel('ay_real')\n",
    "ax5.set_ylabel('ay_pred')\n",
    "ax6.scatter(y_test.values[:,5], y_pred[:,5], s= 2)\n",
    "ax6.set_xlabel('az_real')\n",
    "ax6.set_ylabel('az_pred')\n",
    "\n",
    "plt.subplots_adjust(wspace = 1, hspace = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5ea71a30c257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist_1' is not defined"
     ]
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(hist_1.history['loss'], label='train')\n",
    "plt.plot(hist_1.history['val_loss'], label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "\n",
    "kfold = KFold(n_splits=20, shuffle=True, random_state = 500)\n",
    "X = final_data.iloc[:,2:8]\n",
    "Y = final_data.iloc[:,8:]\n",
    "\n",
    "cv_rmse = []\n",
    "cv_mae = []\n",
    "cv_r2 = []\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    # evaluate the model\n",
    "    pred = model.predict(X.iloc[test,:])\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(Y.iloc[test,:], pred))\n",
    "    mae = mean_absolute_error(Y.iloc[test,:], pred)\n",
    "    r2 = r2_score(Y.iloc[test,:], pred)\n",
    "    \n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mae.append(mae)\n",
    "    cv_r2.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055404173304510584\n",
      "0.00019024198289931165\n",
      "\n",
      "\n",
      "0.04250425183237194\n",
      "0.00014539494579439666\n",
      "\n",
      "\n",
      "0.9969294744946193\n",
      "2.4147009065977242e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cv_rmse))\n",
    "print(np.std(cv_rmse))\n",
    "print('\\n')\n",
    "\n",
    "print(np.mean(cv_mae))\n",
    "print(np.std(cv_mae))\n",
    "print('\\n')\n",
    "\n",
    "print(np.mean(cv_r2))\n",
    "print(np.std(cv_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 scale 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/ordered_molecules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_mean = np.mean(raw_data['px'])\n",
    "px_std = np.std(raw_data['px'])\n",
    "py_mean = np.mean(raw_data['py'])\n",
    "py_std = np.std(raw_data['py'])\n",
    "pz_mean = np.mean(raw_data['pz'])\n",
    "pz_std = np.std(raw_data['pz'])\n",
    "\n",
    "ax_mean = np.mean(raw_data['ax'])\n",
    "ax_std = np.std(raw_data['ax'])\n",
    "ay_mean = np.mean(raw_data['ay'])\n",
    "ay_std = np.std(raw_data['ay'])\n",
    "az_mean = np.mean(raw_data['az'])\n",
    "az_std = np.std(raw_data['az'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.iloc[:,0] = y_test.iloc[:,0] * px_std + px_mean\n",
    "y_test.iloc[:,1] = y_test.iloc[:,1] * py_std + py_mean\n",
    "y_test.iloc[:,2] = y_test.iloc[:,2] * pz_std + pz_mean\n",
    "\n",
    "y_test.iloc[:,3] = y_test.iloc[:,3] * ax_std + ax_mean\n",
    "y_test.iloc[:,4] = y_test.iloc[:,4] * ay_std + ay_mean\n",
    "y_test.iloc[:,5] = y_test.iloc[:,5] * az_std + az_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.iloc[:,0] = y_pred_df.iloc[:,0] * px_std + px_mean\n",
    "y_pred_df.iloc[:,1] = y_pred_df.iloc[:,1] * py_std + py_mean\n",
    "y_pred_df.iloc[:,2] = y_pred_df.iloc[:,2] * pz_std + pz_mean\n",
    "\n",
    "y_pred_df.iloc[:,3] = y_pred_df.iloc[:,3] * ax_std + ax_mean\n",
    "y_pred_df.iloc[:,4] = y_pred_df.iloc[:,4] * ay_std + ay_mean\n",
    "y_pred_df.iloc[:,5] = y_pred_df.iloc[:,5] * az_std + az_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_df)\n",
    "rmse = np.sqrt(mse)\n",
    "mae_metric = mean_absolute_error(y_test, y_pred_df)\n",
    "r2_error = r2_score(y_test, y_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.029494313215668717\n",
      "RMSE:  0.17173908470604096\n",
      "MAE:  0.13471714748190103\n",
      "R2_Score: 0.9969266681155732\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae_metric)\n",
    "print(\"R2_Score:\", r2_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216451</th>\n",
       "      <td>1.93464</td>\n",
       "      <td>2.77564</td>\n",
       "      <td>7.19014</td>\n",
       "      <td>5.052557</td>\n",
       "      <td>3.410067</td>\n",
       "      <td>3.556575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36605</th>\n",
       "      <td>12.34047</td>\n",
       "      <td>11.87384</td>\n",
       "      <td>7.53528</td>\n",
       "      <td>0.326219</td>\n",
       "      <td>-0.453539</td>\n",
       "      <td>0.243936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200500</th>\n",
       "      <td>2.33578</td>\n",
       "      <td>4.88586</td>\n",
       "      <td>6.10077</td>\n",
       "      <td>-1.869792</td>\n",
       "      <td>3.709122</td>\n",
       "      <td>1.326076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121391</th>\n",
       "      <td>3.43433</td>\n",
       "      <td>3.17563</td>\n",
       "      <td>10.90524</td>\n",
       "      <td>-1.457747</td>\n",
       "      <td>2.263259</td>\n",
       "      <td>0.815057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243246</th>\n",
       "      <td>11.64829</td>\n",
       "      <td>3.82942</td>\n",
       "      <td>8.38510</td>\n",
       "      <td>-2.036278</td>\n",
       "      <td>3.493320</td>\n",
       "      <td>-4.187458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186328</th>\n",
       "      <td>4.77522</td>\n",
       "      <td>8.17631</td>\n",
       "      <td>1.00259</td>\n",
       "      <td>-0.963637</td>\n",
       "      <td>0.891893</td>\n",
       "      <td>-1.005037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136623</th>\n",
       "      <td>3.08917</td>\n",
       "      <td>3.43311</td>\n",
       "      <td>10.68677</td>\n",
       "      <td>1.300316</td>\n",
       "      <td>2.504022</td>\n",
       "      <td>-5.222736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208250</th>\n",
       "      <td>10.87436</td>\n",
       "      <td>10.45825</td>\n",
       "      <td>2.80358</td>\n",
       "      <td>2.489527</td>\n",
       "      <td>0.294082</td>\n",
       "      <td>0.015685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30757</th>\n",
       "      <td>6.34425</td>\n",
       "      <td>2.06444</td>\n",
       "      <td>8.11773</td>\n",
       "      <td>-5.274699</td>\n",
       "      <td>0.369777</td>\n",
       "      <td>-0.015520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198277</th>\n",
       "      <td>1.02985</td>\n",
       "      <td>0.39150</td>\n",
       "      <td>8.97285</td>\n",
       "      <td>1.454083</td>\n",
       "      <td>2.734588</td>\n",
       "      <td>-2.769155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102133</th>\n",
       "      <td>10.54293</td>\n",
       "      <td>7.04948</td>\n",
       "      <td>9.33755</td>\n",
       "      <td>-0.211266</td>\n",
       "      <td>-6.494719</td>\n",
       "      <td>0.683749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97224</th>\n",
       "      <td>7.30114</td>\n",
       "      <td>3.14577</td>\n",
       "      <td>1.05556</td>\n",
       "      <td>-0.149660</td>\n",
       "      <td>-0.671146</td>\n",
       "      <td>3.695520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240652</th>\n",
       "      <td>3.35021</td>\n",
       "      <td>11.89788</td>\n",
       "      <td>6.56687</td>\n",
       "      <td>-5.635855</td>\n",
       "      <td>4.234115</td>\n",
       "      <td>-0.475912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82389</th>\n",
       "      <td>6.35850</td>\n",
       "      <td>9.00581</td>\n",
       "      <td>7.48418</td>\n",
       "      <td>-0.153269</td>\n",
       "      <td>-0.416654</td>\n",
       "      <td>-1.560217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237970</th>\n",
       "      <td>3.88611</td>\n",
       "      <td>6.09859</td>\n",
       "      <td>2.27510</td>\n",
       "      <td>-1.673612</td>\n",
       "      <td>-3.960842</td>\n",
       "      <td>2.202040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>1.49519</td>\n",
       "      <td>1.79257</td>\n",
       "      <td>1.59863</td>\n",
       "      <td>1.242727</td>\n",
       "      <td>-4.157950</td>\n",
       "      <td>1.090545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190672</th>\n",
       "      <td>0.59815</td>\n",
       "      <td>9.58208</td>\n",
       "      <td>11.26731</td>\n",
       "      <td>0.650582</td>\n",
       "      <td>-0.083025</td>\n",
       "      <td>0.226974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121991</th>\n",
       "      <td>12.48244</td>\n",
       "      <td>0.20014</td>\n",
       "      <td>12.36302</td>\n",
       "      <td>-1.087970</td>\n",
       "      <td>0.822611</td>\n",
       "      <td>-0.841878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199600</th>\n",
       "      <td>2.45383</td>\n",
       "      <td>7.41998</td>\n",
       "      <td>1.34994</td>\n",
       "      <td>-1.420600</td>\n",
       "      <td>-0.170811</td>\n",
       "      <td>1.015589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99509</th>\n",
       "      <td>2.55232</td>\n",
       "      <td>8.91548</td>\n",
       "      <td>8.02690</td>\n",
       "      <td>-0.907123</td>\n",
       "      <td>-2.217751</td>\n",
       "      <td>1.774204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249250</th>\n",
       "      <td>5.56027</td>\n",
       "      <td>1.74823</td>\n",
       "      <td>3.92777</td>\n",
       "      <td>1.549187</td>\n",
       "      <td>-1.392036</td>\n",
       "      <td>3.217844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89130</th>\n",
       "      <td>4.41986</td>\n",
       "      <td>2.17914</td>\n",
       "      <td>1.27226</td>\n",
       "      <td>-1.271670</td>\n",
       "      <td>0.966736</td>\n",
       "      <td>0.251242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63508</th>\n",
       "      <td>12.79450</td>\n",
       "      <td>6.44336</td>\n",
       "      <td>6.27465</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>-0.939726</td>\n",
       "      <td>-0.443967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67685</th>\n",
       "      <td>12.28758</td>\n",
       "      <td>0.82029</td>\n",
       "      <td>7.25956</td>\n",
       "      <td>-2.240287</td>\n",
       "      <td>1.588809</td>\n",
       "      <td>-1.343427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128499</th>\n",
       "      <td>12.44740</td>\n",
       "      <td>8.27381</td>\n",
       "      <td>3.87336</td>\n",
       "      <td>1.261683</td>\n",
       "      <td>-2.666175</td>\n",
       "      <td>2.727162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6159</th>\n",
       "      <td>2.40567</td>\n",
       "      <td>4.84166</td>\n",
       "      <td>11.45953</td>\n",
       "      <td>0.515194</td>\n",
       "      <td>-1.157168</td>\n",
       "      <td>-1.040382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>11.37109</td>\n",
       "      <td>4.87333</td>\n",
       "      <td>4.56514</td>\n",
       "      <td>1.775627</td>\n",
       "      <td>0.799405</td>\n",
       "      <td>-3.102032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142584</th>\n",
       "      <td>8.73379</td>\n",
       "      <td>9.45450</td>\n",
       "      <td>1.09546</td>\n",
       "      <td>-4.862045</td>\n",
       "      <td>0.294611</td>\n",
       "      <td>1.029928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132860</th>\n",
       "      <td>9.95248</td>\n",
       "      <td>7.84569</td>\n",
       "      <td>5.47158</td>\n",
       "      <td>3.331826</td>\n",
       "      <td>-4.299482</td>\n",
       "      <td>-0.087569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164922</th>\n",
       "      <td>6.27076</td>\n",
       "      <td>9.26014</td>\n",
       "      <td>4.53319</td>\n",
       "      <td>-1.257086</td>\n",
       "      <td>1.448758</td>\n",
       "      <td>-0.789012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70569</th>\n",
       "      <td>4.16982</td>\n",
       "      <td>4.98647</td>\n",
       "      <td>2.38078</td>\n",
       "      <td>2.214945</td>\n",
       "      <td>0.418790</td>\n",
       "      <td>-2.370936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64094</th>\n",
       "      <td>7.89924</td>\n",
       "      <td>9.56800</td>\n",
       "      <td>9.88248</td>\n",
       "      <td>-1.301608</td>\n",
       "      <td>-1.960757</td>\n",
       "      <td>3.332477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128250</th>\n",
       "      <td>10.70767</td>\n",
       "      <td>9.27877</td>\n",
       "      <td>2.95824</td>\n",
       "      <td>-4.831772</td>\n",
       "      <td>-1.376877</td>\n",
       "      <td>-2.396213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12555</th>\n",
       "      <td>1.35756</td>\n",
       "      <td>5.17996</td>\n",
       "      <td>5.15609</td>\n",
       "      <td>-0.335251</td>\n",
       "      <td>-0.522598</td>\n",
       "      <td>-0.178002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103851</th>\n",
       "      <td>3.90744</td>\n",
       "      <td>5.87440</td>\n",
       "      <td>3.49743</td>\n",
       "      <td>-0.519209</td>\n",
       "      <td>3.625942</td>\n",
       "      <td>0.133293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200515</th>\n",
       "      <td>7.99868</td>\n",
       "      <td>1.11951</td>\n",
       "      <td>7.68627</td>\n",
       "      <td>-2.971656</td>\n",
       "      <td>-1.077866</td>\n",
       "      <td>-4.383014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193467</th>\n",
       "      <td>8.74360</td>\n",
       "      <td>10.46936</td>\n",
       "      <td>4.18358</td>\n",
       "      <td>0.936675</td>\n",
       "      <td>-3.130028</td>\n",
       "      <td>-0.649302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98058</th>\n",
       "      <td>2.97264</td>\n",
       "      <td>2.24060</td>\n",
       "      <td>3.23014</td>\n",
       "      <td>3.366357</td>\n",
       "      <td>1.670913</td>\n",
       "      <td>-0.791301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212508</th>\n",
       "      <td>0.55132</td>\n",
       "      <td>8.74570</td>\n",
       "      <td>4.97655</td>\n",
       "      <td>-4.889916</td>\n",
       "      <td>1.451590</td>\n",
       "      <td>5.452949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148667</th>\n",
       "      <td>9.25500</td>\n",
       "      <td>10.09272</td>\n",
       "      <td>4.30566</td>\n",
       "      <td>-2.688076</td>\n",
       "      <td>0.733922</td>\n",
       "      <td>0.800223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247112</th>\n",
       "      <td>7.14521</td>\n",
       "      <td>3.48833</td>\n",
       "      <td>11.69833</td>\n",
       "      <td>-2.618478</td>\n",
       "      <td>-2.836374</td>\n",
       "      <td>-1.928660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50907</th>\n",
       "      <td>7.90359</td>\n",
       "      <td>10.89013</td>\n",
       "      <td>5.20549</td>\n",
       "      <td>2.795548</td>\n",
       "      <td>-0.159479</td>\n",
       "      <td>3.327156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102247</th>\n",
       "      <td>10.10403</td>\n",
       "      <td>0.65091</td>\n",
       "      <td>0.24535</td>\n",
       "      <td>2.940540</td>\n",
       "      <td>-0.120427</td>\n",
       "      <td>-1.084222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10348</th>\n",
       "      <td>9.90869</td>\n",
       "      <td>3.31039</td>\n",
       "      <td>6.37366</td>\n",
       "      <td>0.369741</td>\n",
       "      <td>1.625293</td>\n",
       "      <td>0.472082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149064</th>\n",
       "      <td>9.04250</td>\n",
       "      <td>4.33883</td>\n",
       "      <td>0.99219</td>\n",
       "      <td>0.324513</td>\n",
       "      <td>2.551614</td>\n",
       "      <td>-3.850024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>10.06471</td>\n",
       "      <td>3.15939</td>\n",
       "      <td>6.97839</td>\n",
       "      <td>-2.466370</td>\n",
       "      <td>1.795675</td>\n",
       "      <td>-2.819136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44966</th>\n",
       "      <td>3.32317</td>\n",
       "      <td>1.05051</td>\n",
       "      <td>10.13378</td>\n",
       "      <td>0.939011</td>\n",
       "      <td>0.838944</td>\n",
       "      <td>-0.188186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189657</th>\n",
       "      <td>6.92737</td>\n",
       "      <td>11.42039</td>\n",
       "      <td>2.05208</td>\n",
       "      <td>1.754467</td>\n",
       "      <td>5.326996</td>\n",
       "      <td>-1.690361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142947</th>\n",
       "      <td>10.73196</td>\n",
       "      <td>2.85667</td>\n",
       "      <td>5.27833</td>\n",
       "      <td>-0.261461</td>\n",
       "      <td>-0.209699</td>\n",
       "      <td>2.680888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247922</th>\n",
       "      <td>8.83421</td>\n",
       "      <td>4.13921</td>\n",
       "      <td>0.13311</td>\n",
       "      <td>2.679928</td>\n",
       "      <td>-4.497436</td>\n",
       "      <td>-3.539767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73352</th>\n",
       "      <td>11.90275</td>\n",
       "      <td>4.04616</td>\n",
       "      <td>12.18123</td>\n",
       "      <td>1.245243</td>\n",
       "      <td>-3.204792</td>\n",
       "      <td>1.735993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162939</th>\n",
       "      <td>11.28016</td>\n",
       "      <td>10.67211</td>\n",
       "      <td>5.69475</td>\n",
       "      <td>-0.618940</td>\n",
       "      <td>-1.871153</td>\n",
       "      <td>1.301019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47643</th>\n",
       "      <td>1.71557</td>\n",
       "      <td>11.06216</td>\n",
       "      <td>4.93306</td>\n",
       "      <td>-1.131668</td>\n",
       "      <td>-4.102278</td>\n",
       "      <td>-1.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65423</th>\n",
       "      <td>1.46973</td>\n",
       "      <td>3.00839</td>\n",
       "      <td>10.40521</td>\n",
       "      <td>0.963222</td>\n",
       "      <td>5.363598</td>\n",
       "      <td>2.077667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204751</th>\n",
       "      <td>6.82393</td>\n",
       "      <td>7.82788</td>\n",
       "      <td>0.38712</td>\n",
       "      <td>2.816666</td>\n",
       "      <td>-8.221271</td>\n",
       "      <td>6.657826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202158</th>\n",
       "      <td>11.46080</td>\n",
       "      <td>5.00892</td>\n",
       "      <td>7.41328</td>\n",
       "      <td>-2.023981</td>\n",
       "      <td>-6.770340</td>\n",
       "      <td>4.890616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197124</th>\n",
       "      <td>1.07527</td>\n",
       "      <td>10.61458</td>\n",
       "      <td>7.78038</td>\n",
       "      <td>0.916985</td>\n",
       "      <td>-0.415645</td>\n",
       "      <td>-0.882937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>4.53850</td>\n",
       "      <td>10.87982</td>\n",
       "      <td>1.66863</td>\n",
       "      <td>1.705384</td>\n",
       "      <td>2.569661</td>\n",
       "      <td>-1.552762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171347</th>\n",
       "      <td>6.94407</td>\n",
       "      <td>7.64290</td>\n",
       "      <td>6.12622</td>\n",
       "      <td>0.583092</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>-0.897434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158371</th>\n",
       "      <td>4.03995</td>\n",
       "      <td>1.57832</td>\n",
       "      <td>4.89825</td>\n",
       "      <td>3.015071</td>\n",
       "      <td>2.575706</td>\n",
       "      <td>0.963394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76664 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              8         9         10        11        12        13\n",
       "216451   1.93464   2.77564   7.19014  5.052557  3.410067  3.556575\n",
       "36605   12.34047  11.87384   7.53528  0.326219 -0.453539  0.243936\n",
       "200500   2.33578   4.88586   6.10077 -1.869792  3.709122  1.326076\n",
       "121391   3.43433   3.17563  10.90524 -1.457747  2.263259  0.815057\n",
       "243246  11.64829   3.82942   8.38510 -2.036278  3.493320 -4.187458\n",
       "186328   4.77522   8.17631   1.00259 -0.963637  0.891893 -1.005037\n",
       "136623   3.08917   3.43311  10.68677  1.300316  2.504022 -5.222736\n",
       "208250  10.87436  10.45825   2.80358  2.489527  0.294082  0.015685\n",
       "30757    6.34425   2.06444   8.11773 -5.274699  0.369777 -0.015520\n",
       "198277   1.02985   0.39150   8.97285  1.454083  2.734588 -2.769155\n",
       "102133  10.54293   7.04948   9.33755 -0.211266 -6.494719  0.683749\n",
       "97224    7.30114   3.14577   1.05556 -0.149660 -0.671146  3.695520\n",
       "240652   3.35021  11.89788   6.56687 -5.635855  4.234115 -0.475912\n",
       "82389    6.35850   9.00581   7.48418 -0.153269 -0.416654 -1.560217\n",
       "237970   3.88611   6.09859   2.27510 -1.673612 -3.960842  2.202040\n",
       "8193     1.49519   1.79257   1.59863  1.242727 -4.157950  1.090545\n",
       "190672   0.59815   9.58208  11.26731  0.650582 -0.083025  0.226974\n",
       "121991  12.48244   0.20014  12.36302 -1.087970  0.822611 -0.841878\n",
       "199600   2.45383   7.41998   1.34994 -1.420600 -0.170811  1.015589\n",
       "99509    2.55232   8.91548   8.02690 -0.907123 -2.217751  1.774204\n",
       "249250   5.56027   1.74823   3.92777  1.549187 -1.392036  3.217844\n",
       "89130    4.41986   2.17914   1.27226 -1.271670  0.966736  0.251242\n",
       "63508   12.79450   6.44336   6.27465  0.073800 -0.939726 -0.443967\n",
       "67685   12.28758   0.82029   7.25956 -2.240287  1.588809 -1.343427\n",
       "128499  12.44740   8.27381   3.87336  1.261683 -2.666175  2.727162\n",
       "6159     2.40567   4.84166  11.45953  0.515194 -1.157168 -1.040382\n",
       "22123   11.37109   4.87333   4.56514  1.775627  0.799405 -3.102032\n",
       "142584   8.73379   9.45450   1.09546 -4.862045  0.294611  1.029928\n",
       "132860   9.95248   7.84569   5.47158  3.331826 -4.299482 -0.087569\n",
       "164922   6.27076   9.26014   4.53319 -1.257086  1.448758 -0.789012\n",
       "...          ...       ...       ...       ...       ...       ...\n",
       "70569    4.16982   4.98647   2.38078  2.214945  0.418790 -2.370936\n",
       "64094    7.89924   9.56800   9.88248 -1.301608 -1.960757  3.332477\n",
       "128250  10.70767   9.27877   2.95824 -4.831772 -1.376877 -2.396213\n",
       "12555    1.35756   5.17996   5.15609 -0.335251 -0.522598 -0.178002\n",
       "103851   3.90744   5.87440   3.49743 -0.519209  3.625942  0.133293\n",
       "200515   7.99868   1.11951   7.68627 -2.971656 -1.077866 -4.383014\n",
       "193467   8.74360  10.46936   4.18358  0.936675 -3.130028 -0.649302\n",
       "98058    2.97264   2.24060   3.23014  3.366357  1.670913 -0.791301\n",
       "212508   0.55132   8.74570   4.97655 -4.889916  1.451590  5.452949\n",
       "148667   9.25500  10.09272   4.30566 -2.688076  0.733922  0.800223\n",
       "247112   7.14521   3.48833  11.69833 -2.618478 -2.836374 -1.928660\n",
       "50907    7.90359  10.89013   5.20549  2.795548 -0.159479  3.327156\n",
       "102247  10.10403   0.65091   0.24535  2.940540 -0.120427 -1.084222\n",
       "10348    9.90869   3.31039   6.37366  0.369741  1.625293  0.472082\n",
       "149064   9.04250   4.33883   0.99219  0.324513  2.551614 -3.850024\n",
       "3948    10.06471   3.15939   6.97839 -2.466370  1.795675 -2.819136\n",
       "44966    3.32317   1.05051  10.13378  0.939011  0.838944 -0.188186\n",
       "189657   6.92737  11.42039   2.05208  1.754467  5.326996 -1.690361\n",
       "142947  10.73196   2.85667   5.27833 -0.261461 -0.209699  2.680888\n",
       "247922   8.83421   4.13921   0.13311  2.679928 -4.497436 -3.539767\n",
       "73352   11.90275   4.04616  12.18123  1.245243 -3.204792  1.735993\n",
       "162939  11.28016  10.67211   5.69475 -0.618940 -1.871153  1.301019\n",
       "47643    1.71557  11.06216   4.93306 -1.131668 -4.102278 -1.994064\n",
       "65423    1.46973   3.00839  10.40521  0.963222  5.363598  2.077667\n",
       "204751   6.82393   7.82788   0.38712  2.816666 -8.221271  6.657826\n",
       "202158  11.46080   5.00892   7.41328 -2.023981 -6.770340  4.890616\n",
       "197124   1.07527  10.61458   7.78038  0.916985 -0.415645 -0.882937\n",
       "3385     4.53850  10.87982   1.66863  1.705384  2.569661 -1.552762\n",
       "171347   6.94407   7.64290   6.12622  0.583092  0.938017 -0.897434\n",
       "158371   4.03995   1.57832   4.89825  3.015071  2.575706  0.963394\n",
       "\n",
       "[76664 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/env_py3/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "\n",
    "kfold = KFold(n_splits=20, shuffle=True, random_state = 500)\n",
    "X = final_data.iloc[:,2:8]\n",
    "Y = final_data.iloc[:,8:]\n",
    "\n",
    "cv_rmse = []\n",
    "cv_mae = []\n",
    "cv_r2 = []\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    y_test = Y.iloc[test,:]\n",
    "\n",
    "    y_test.iloc[:,0] = y_test.iloc[:,0] * px_std + px_mean\n",
    "    y_test.iloc[:,1] = y_test.iloc[:,1] * py_std + py_mean\n",
    "    y_test.iloc[:,2] = y_test.iloc[:,2] * pz_std + pz_mean\n",
    "\n",
    "    y_test.iloc[:,3] = y_test.iloc[:,3] * ax_std + ax_mean\n",
    "    y_test.iloc[:,4] = y_test.iloc[:,4] * ay_std + ay_mean\n",
    "    y_test.iloc[:,5] = y_test.iloc[:,5] * az_std + az_mean\n",
    "    \n",
    "    y_pred = model.predict(X.iloc[test,:])\n",
    "    y_pred_df = pd.DataFrame(y_pred)\n",
    "    \n",
    "    y_pred_df.iloc[:,0] = y_pred_df.iloc[:,0] * px_std + px_mean\n",
    "    y_pred_df.iloc[:,1] = y_pred_df.iloc[:,1] * py_std + py_mean\n",
    "    y_pred_df.iloc[:,2] = y_pred_df.iloc[:,2] * pz_std + pz_mean\n",
    "\n",
    "    y_pred_df.iloc[:,3] = y_pred_df.iloc[:,3] * ax_std + ax_mean\n",
    "    y_pred_df.iloc[:,4] = y_pred_df.iloc[:,4] * ay_std + ay_mean\n",
    "    y_pred_df.iloc[:,5] = y_pred_df.iloc[:,5] * az_std + az_mean\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_df))\n",
    "    mae = mean_absolute_error(y_test, y_pred_df)\n",
    "    r2 = r2_score(y_test, y_pred_df)\n",
    "    \n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mae.append(mae)\n",
    "    cv_r2.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1717292540241969\n",
      "0.0005681295826024924\n",
      "\n",
      "\n",
      "0.1346639098858025\n",
      "0.0004554075140078578\n",
      "\n",
      "\n",
      "0.9969294738058411\n",
      "2.4147014961791257e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cv_rmse))\n",
    "print(np.std(cv_rmse))\n",
    "print('\\n')\n",
    "\n",
    "print(np.mean(cv_mae))\n",
    "print(np.std(cv_mae))\n",
    "print('\\n')\n",
    "\n",
    "print(np.mean(cv_r2))\n",
    "print(np.std(cv_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
